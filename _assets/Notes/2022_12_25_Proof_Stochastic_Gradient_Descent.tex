%% LyX 2.3.6.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[12pt,english,nohyper]{tufte-handout}
\usepackage{helvet}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{babel}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\PassOptionsToPackage{normalem}{ulem}
\usepackage{ulem}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=1,
 breaklinks=true,pdfborder={0 0 0},pdfborderstyle={},backref=false,colorlinks=false]
 {hyperref}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\floatstyle{ruled}
\newfloat{algorithm}{tbp}{loa}
\providecommand{\algorithmname}{Algorithm}
\floatname{algorithm}{\protect\algorithmname}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{ stmaryrd }
\usepackage{ textcomp }
\newtheorem{assumption}{Assumption}


\newcommand{\filler}[1][10]%
{   \foreach \x in {1,...,#1}
    {   test 
    }
}

\def\mathnote#1{%
  \tag*{\rlap{\hspace\marginparsep\smash{\parbox[t]{\marginparwidth}{%
  \footnotesize#1}}}}
}

\makeatother

\begin{document}

\section{Algorithm description}

\subsection*{Problem setup}

We are interested in solving the problem\marginnote{\textbf{Notation}

$\Pi_{C}$: projection onto the set $C$}
\begin{equation}
p^{\star}=\left(\begin{array}{ll}
\underset{x\in\mathbb{R}^{d}}{\mbox{minimize}} & f(x)\\
\textrm{subject to} & x\in C,
\end{array}\right)\tag{\ensuremath{\mathcal{P}}}\label{eq:problem}
\end{equation}
where we have the following assumptions regarding the nature of the
problem.

\subsection*{Assumption 1}

We assume:
\begin{itemize}
\item $f:\mathbb{R}^{d}\to(-\infty,\infty]$ is a closed, proper, and convex
function, 
\item $C$ is a nonempty, closed, convex set, with $C\subseteq\textrm{int}\textrm{dom\,}f$,
and 
\item $\textrm{argmin}f(C)=X^{\star}\neq\emptyset$. 
\end{itemize}

\subsection*{Stochastic gradient descent}

The stochastic gradient descent (SGD) algorithm to solve \eqref{eq:problem}
is described by Algorithm \ref{alg:sgd}, where we make the following
assumption regarding the nature of the oracle.

\subsection*{Assumption 2}

We assume that given an iterate $x_{k}$, the stochastic oracle is
capable of producing a random vector $g_{k}$with the following properties:
\begin{itemize}
\item (unbiased) $\forall_{k\geq0}\;\mathbf{E}\left[g_{k}\mid x_{k}\right]\in\partial f(x_{k})$,
and
\item (bounded variance) $\exists_{G>0}\;\forall_{k\geq0}\;\mathbf{E}\left[\|g_{k}\|^{2}\mid x_{k}\right]\leq G^{2}.$
\end{itemize}
\begin{algorithm}[h]
{\small{}\hrulefill}{\small\par}

{\small{}\hrulefill}{\small\par}

\textbf{\small{}\uline{input:}}\textbf{\small{} }{\small{}$f$,
$C$, iteration limit $K$}{\small\par}

{\small{}\hrulefill}{\small\par}

\textbf{\small{}\uline{algorithm:}}{\small\par}

\textbf{\small{}1. initialization}{\small{}:}{\small\par}

{\small{}pick $x_{0}\in C$ arbitrarily}{\small\par}

\textbf{\small{}2. main iteration:}{\small\par}

\textbf{\small{}for}{\small{} $k=0,1,2,\ldots,K-1$}{\small\par}

{\small{}$\quad$pick stepsizes $\alpha_{k}>0$ and random $g_{k}\in\mathbb{R}^{d}$
satisfying Assumption \ref{assum:oracle}}{\small\par}

$\quad$$x_{k+1}\gets\Pi_{C}\left(x_{k}-\alpha_{k}g_{k}\right)$

\textbf{\small{}end for}{\small\par}

\textbf{\small{}3. return}{\small{} }\textbf{\small{}$x_{K}$}{\small\par}

{\small{}\hrulefill}\caption{SGD to solve \eqref{eq:problem}}
\label{alg:sgd}
\end{algorithm}
 

\subsection*{Convergence analysis}

First, note that, for all $k\geq0$:

\begin{align*}
 & \mathbf{E}\left[\|\overbrace{x_{k+1}}^{=\Pi_{C}(x_{k}-\alpha_{k}g_{k})}-\overbrace{x_{\star}}^{=\Pi_{C}(x_{\star})}\|^{2}\mid x_{k}\right]\\
= & \mathbf{E}\left[\overbrace{\|\Pi_{C}\left(x_{k}-\alpha_{k}g_{k}\right)-\Pi_{C}(x_{\star})\|^{2}}^{\leq\|x_{k}-\alpha_{k}g_{k}-x_{\star}\|^{2}}\mid x_{k}\right]\\
\leq & \mathbf{E}\left[\overbrace{\|x_{k}-\alpha_{k}g_{k}-x_{\star}\|^{2}}^{=\|x_{k}-x_{\star}\|^{2}+\alpha_{k}^{2}\|g_{k}\|^{2}-2\alpha_{k}\left\langle x_{k}-x_{\star};\,g_{k}\right\rangle }\mid x_{k}\right]\\
= & \mathbf{E}\left[\|x_{k}-x_{\star}\|^{2}+\alpha_{k}^{2}\|g_{k}\|^{2}-2\alpha_{k}\left\langle x_{k}-x_{\star};\,g_{k}\right\rangle \mid x_{k}\right]\\
= & \mathbf{E}\left[\|x_{k}-x_{\star}\|^{2}\mid x_{k}\right]+\alpha_{k}^{2}\mathbf{E}\left[\|g_{k}\|^{2}\mid x_{k}\right]-2\alpha_{k}\mathbf{E}\left[\left\langle x_{k}-x_{\star};\,g_{k}\right\rangle \mid x_{k}\right]\mathnote{\textrm{\ensuremath{\rhd\;}using linearity of expectation}}\\
= & \|x_{k}-x_{\star}\|^{2}+\alpha_{k}^{2}\mathbf{E}\left[\|g_{k}\|^{2}\mid x_{k}\right]-2\alpha_{k}\left\langle x_{k}-x_{\star};\,\mathbf{E}\left[g_{k}\mid x_{k}\right]\right\rangle \mathnote{\textrm{\ensuremath{\rhd\;}using "taking out what's known" rule  \ensuremath{\;\mathbf{E}\left[h(X)Y\mid X\right]=h(X)\mathbf{E}\left[Y\mid X\right]}}}\\
\leq & \|x_{k}-x_{\star}\|^{2}+\alpha_{k}^{2}G^{2}-2\alpha_{k}\left\langle x_{k}-x_{\star};\,\mathbf{E}\left[g_{k}\mid x_{k}\right]\right\rangle \\
 & \quad\texttt{/*}\\
 & \quad\textrm{we have }\mathbf{E}\left[\|g_{k}\|^{2}\mid x_{k}\right]\ensuremath{\leq G^{2}}\\
 & \quad\Leftrightarrow\forall_{y}\;f(y)\geq f(x_{k})+\left\langle \mathbf{E}\left[g_{k}\mid x_{k}\right];\,y-x_{k}\right\rangle \\
 & \quad\overset{y\gets x_{\star}}{\Rightarrow}f(x_{\star})\geq f(x_{k})-\left\langle \mathbf{E}\left[g_{k}\mid x_{k}\right];\,x_{k}-x_{\star}\right\rangle \\
 & \quad\ensuremath{\Rightarrow-\left\langle \mathbf{E}\left[g_{k}\mid x_{k}\right];\,x_{k}-x_{\star}\right\rangle \leq f(x_{\star})-f(x_{k})}\\
 & \quad\texttt{*/}\\
= & \|x_{k}-x_{\star}\|^{2}+\alpha_{k}^{2}G^{2}-2\alpha_{k}\left(f(x_{k})-f(x_{\star})\right),
\end{align*}
 So, we have proved
\[
\mathbf{E}\left[\|x_{k+1}-x_{\star}\|^{2}\mid x_{k}\right]\leq\|x_{k}-x_{\star}\|^{2}+\alpha_{k}^{2}G^{2}-2\alpha_{k}\left(f(x_{k})-f(x_{\star})\right),
\]
 so taking expectation with respect to $x_{k}$ on both sides, we
get: 
\begin{align*}
 & \mathbf{E}\left[\mathbf{E}\left[\|x_{k+1}-x_{\star}\|^{2}\mid x_{k}\right]\right]\\
= & \mathbf{E}\left[\|x_{k+1}-x_{\star}\|^{2}\right]\mathnote{\textrm{\ensuremath{\rhd\;}using Adam's law\ensuremath{\mathbf{E}\left[\mathbf{E}\left[Y\mid X\right]\right]=\mathbf{E}\left[Y\right]}}}\\
\leq & \mathbf{E}\left[\|x_{k}-x_{\star}\|^{2}+\alpha_{k}^{2}G^{2}-2\alpha_{k}\left(f(x_{k})-f(x_{\star})\right)\right]\\
= & \mathbf{E}\left[\|x_{k}-x_{\star}\|^{2}\right]-2\alpha_{k}\mathbf{E}\left[f(x_{k})-f(x_{\star})\right]+\alpha_{k}^{2}G^{2},
\end{align*}
so 
\[
\mathbf{E}\left[\|x_{k+1}-x_{\star}\|^{2}\right]-\mathbf{E}\left[\|x_{k}-x_{\star}\|^{2}\right]\leq-2\alpha_{k}\mathbf{E}\left[f(x_{k})-f(x_{\star})\right]+\alpha_{k}^{2}G^{2}.
\]
Now, let us do a telescoping sum:
\begin{align*}
\mathbf{E}\left[\|x_{k+1}-x_{\star}\|^{2}\right]-\mathbf{E}\left[\|x_{k}-x_{\star}\|^{2}\right] & \leq-2\alpha_{k}\mathbf{E}\left[f(x_{k})-f(x_{\star})\right]+\alpha_{k}^{2}G^{2}\\
\mathbf{E}\left[\|x_{k}-x_{\star}\|^{2}\right]-\mathbf{E}\left[\|x_{k-1}-x_{\star}\|^{2}\right] & \leq-2\alpha_{k}\mathbf{E}\left[f(x_{k-1})-f(x_{\star})\right]+\alpha_{k-1}^{2}G^{2}\\
\vdots & \vdots\\
\mathbf{E}\left[\|x_{m+1}-x_{\star}\|^{2}\right]-\mathbf{E}\left[\|x_{m}-x_{\star}\|^{2}\right] & \leq-2\alpha_{m}\mathbf{E}\left[f(x_{m})-f(x_{\star})\right]+\alpha_{m}^{2}G^{2},
\end{align*}
 and adding the equations above, we get: 
\begin{align*}
 & \mathbf{E}\left[\|x_{k+1}-x_{\star}\|^{2}\right]-\mathbf{E}\left[\|x_{m}-x_{\star}\|^{2}\right]\leq-2\sum_{i=m}^{k}\alpha_{i}\mathbf{E}\left[f(x_{i})-f(x_{\star})\right]+G^{2}\sum_{i=m}^{k}\alpha_{i}^{2}\\
\Leftrightarrow & 0\leq\mathbf{E}\left[\|x_{k+1}-x_{\star}\|^{2}\right]\leq\mathbf{E}\left[\|x_{m}-x_{\star}\|^{2}\right]-2\sum_{i=m}^{k}\alpha_{i}\mathbf{E}\left[f(x_{i})-f(x_{\star})\right]+G^{2}\sum_{i=m}^{k}\alpha_{i}^{2}\\
\Rightarrow & 0\leq\mathbf{E}\left[\|x_{m}-x_{\star}\|^{2}\right]-2\sum_{i=m}^{k}\alpha_{i}\mathbf{E}\left[f(x_{i})-f(x_{\star})\right]+G^{2}\sum_{i=1}^{m}\alpha_{i}^{2}\\
\Leftrightarrow & \sum_{i=m}^{k}\alpha_{i}\mathbf{E}\left[f(x_{i})-f(x_{\star})\right]\leq\frac{1}{2}\left(\mathbf{E}\left[\|x_{m}-x_{\star}\|^{2}\right]+G^{2}\sum_{i=m}^{k}\alpha_{i}^{2}\right)\\
\Rightarrow & \left(\sum_{i=m}^{k}\alpha_{i}\right)\left(\min_{i\in\{m,\ldots,k\}}\mathbf{E}\left[f(x_{i})-f(x_{\star})\right]\right)\\
 & \quad\leq\frac{1}{2}\left(\mathbf{E}\left[\|x_{m}-x_{\star}\|^{2}\right]+G^{2}\sum_{i=m}^{k}\alpha_{i}^{2}\right)\mathnote{\textrm{\ensuremath{\rhd\;}for \ensuremath{b_{k}\geq0}, we have \ensuremath{\left(\min_{k}a_{k}\right)\sum_{k}b_{k}\leq\sum_{k}a_{k}b_{k}}}}\\
\Rightarrow & \mathbf{E}\left[\min_{i\in\{m,\ldots,k\}}\left\{ f(x_{i})-f(x_{\star})\right\} \right]\leq\min_{i\in\{m,\ldots,k\}}\mathbf{E}\left[f(x_{i})-f(x_{\star})\right]\\
 & \quad\leq\frac{\mathbf{E}\left[\|x_{m}-x_{\star}\|^{2}\right]+G^{2}\sum_{i=m}^{k}\alpha_{i}^{2}}{2\sum_{i=m}^{k}\alpha_{i}}.\mathnote{\textrm{\ensuremath{\rhd\;}using \ensuremath{\mathbf{E}\left[\min_{i}X_{i}\right]\leq\min_{i}\mathbf{E}\left[X_{i}\right]}}}
\end{align*}
In the last inequality, $m$ is arbitrary, so set $m\gets0$, which
leads to: 
\[
\mathbf{E}\left[\min_{i\in\{0,\ldots,k\}}f(x_{i})\right]-f(x_{\star})\leq\frac{\mathbf{E}\left[\|x_{0}-x_{\star}\|^{2}\right]+G^{2}\sum_{i=0}^{k}\alpha_{i}^{2}}{2\sum_{i=0}^{k}\alpha_{i}},
\]
 so if we have $\sum_{i=0}^{k}\alpha_{i}^{2}<\infty$ and $\sum_{i=0}^{k}\alpha_{i}=\infty$,
then we have 
\[
\mathbf{E}\left[\min_{i\in\{0,\ldots,k\}}f(x_{i})\right]\to f(x_{\star}).
\]
 
\end{document}
